{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experts Hook for Mixtral 8x7B\n",
    "\n",
    "This experiment is storing all the tokens of a datasets into a database where I can view which tokens go to which experts and plot paths. Maybe given multiple datasets, I could see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import altair as alt\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "We will use the `Salesforce/wikitext` dataset. Its very simple, unassuming and easily understand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/wikitext\", 'wikitext-2-raw-v1', split=\"test\")\n",
    "data = \"\\n\".join(dataset[\"text\"][:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "We would need to add a forward hook into the layers to extract which experts a token gets send to which experts. To store the information, we will use 2 simple databases right now. \n",
    "- expert_to_token: 32 keys representing the Layers, each contain a dictionary of where keys are the exprts and values are sets of tokens. Tracks which tokens are sent to the expert.\n",
    "- token_to_expert: keys are tokens, values are 32-length lists of 2-tuples. Tracks which experts processed a token through the decoder layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44e26908a104e55b47e00160d9fcdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_prefix_space=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = len(model.layers)\n",
    "token_to_expert = defaultdict(list)\n",
    "expert_to_token = {f\"Layer {l}\": defaultdict(set) for l in range(NUM_LAYERS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixtralModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MixtralDecoderLayer(\n",
      "      (self_attn): MixtralSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MixtralRotaryEmbedding()\n",
      "      )\n",
      "      (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "        (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
      "        (experts): ModuleList(\n",
      "          (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
      "            (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (input_layernorm): MixtralRMSNorm()\n",
      "      (post_attention_layernorm): MixtralRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): MixtralRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "Right now, we just define a hook that prints out which experts were chose for a specific output of the attention heads. We need to understand which tokens are directed where. So I will try and figure out which ones go where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_print_token(prompt: str, layer_num:int):\n",
    "    ids = tokenizer.encode(prompt, add_special_tokens=False) \n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(len(tokens))\n",
    "    pos_to_token = {i: token for i, token in enumerate(tokens)}\n",
    "\n",
    "    def print_tokens(module, input, output):\n",
    "        _, topk_index = t.topk(output, 2, dim=1)\n",
    "        \n",
    "        # topk_list of of the shape [S_l, 2] where S_l is the length of the sequence\n",
    "        topk_list = topk_index.tolist()\n",
    "        print(topk_list)\n",
    "        for i, topk in enumerate(topk_list):\n",
    "            token = pos_to_token[i]\n",
    "            topk = tuple(topk)\n",
    "            token_to_expert[token].append(topk)\n",
    "            expert_1, expert_2 = topk\n",
    "            expert_to_token[f\"Layer {layer_num}\"][expert_1].add(token)\n",
    "            expert_to_token[f\"Layer {layer_num}\"][expert_2].add(token)\n",
    "    return print_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n"
     ]
    }
   ],
   "source": [
    "hooks = []\n",
    "for layer_num, layers in enumerate(model.layers):\n",
    "    hook = layers.block_sparse_moe.gate.register_forward_hook(get_print_token(data, layer_num))\n",
    "    hooks.append(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [5, 0], [5, 6], [2, 5], [1, 0], [2, 5], [0, 4], [1, 0], [2, 5], [0, 5], [5, 6], [5, 6], [5, 2], [1, 5], [2, 5], [0, 4], [1, 4], [6, 4], [3, 2], [1, 7], [1, 0], [3, 5], [1, 0], [3, 7], [1, 7], [0, 7], [3, 5], [3, 2], [4, 5], [3, 2], [1, 0], [5, 6], [3, 5], [5, 3], [3, 5], [4, 5], [1, 0], [5, 7], [2, 5], [1, 0], [0, 1], [2, 5], [1, 2], [5, 4], [2, 5], [6, 7], [4, 7], [4, 7], [4, 7], [3, 5], [6, 5], [6, 4], [2, 0], [5, 7], [3, 5], [3, 1], [4, 7], [1, 0], [5, 4], [2, 6], [0, 4], [2, 5], [1, 4], [4, 0], [5, 2], [1, 0], [1, 4], [0, 1], [3, 5], [6, 5], [6, 5], [4, 7], [5, 4], [2, 5], [6, 7], [4, 7], [4, 7], [4, 5], [7, 5], [2, 5], [1, 7], [1, 7], [1, 7], [3, 5], [3, 5], [4, 5], [3, 5], [1, 5], [1, 0], [5, 4], [2, 5], [1, 0], [1, 0], [1, 0], [1, 0], [2, 6], [4, 7], [5, 4], [2, 5], [6, 7], [4, 7], [4, 7], [7, 4], [3, 5], [5, 6], [2, 5], [6, 7], [4, 7], [4, 7], [4, 7], [2, 5], [0, 4], [1, 7], [4, 1], [3, 2], [0, 7], [7, 0], [2, 5], [1, 0], [2, 5], [5, 2], [2, 6], [0, 1], [2, 5], [2, 6], [0, 4], [6, 2], [4, 5], [1, 0], [5, 2], [7, 5], [2, 3], [1, 0], [0, 1], [2, 3], [3, 0], [2, 3], [0, 4], [3, 5], [3, 4], [3, 1], [4, 7], [7, 5], [4, 5], [1, 0], [0, 7], [3, 7], [2, 6], [1, 2], [4, 1], [0, 7], [3, 5], [3, 5], [6, 4], [1, 7], [5, 4], [2, 0], [2, 5], [6, 7], [4, 7], [4, 7], [4, 7], [1, 4], [2, 4], [5, 4], [7, 0], [2, 3], [1, 0], [5, 2], [4, 2], [4, 1], [0, 4], [2, 5], [1, 4], [1, 0], [3, 5], [6, 5], [6, 5], [4, 7], [7, 5], [2, 5], [2, 5], [4, 1], [1, 7], [5, 0], [2, 5], [0, 4], [0, 4], [3, 7], [2, 5], [1, 3], [4, 0], [2, 7], [1, 7], [1, 7], [5, 7], [0, 7], [3, 5], [3, 5], [6, 4], [4, 6], [5, 2], [1, 0], [2, 5], [3, 4], [0, 4], [3, 5], [3, 1], [4, 7], [2, 5], [1, 0], [2, 3], [4, 7], [0, 1], [3, 5], [2, 3], [1, 4], [2, 5], [0, 1], [3, 5], [1, 2], [1, 0], [3, 5], [2, 3], [1, 4], [1, 3], [0, 1], [3, 2], [1, 0], [6, 2], [1, 0], [3, 5], [2, 5], [4, 0], [0, 7], [3, 1], [0, 7], [5, 6], [6, 5], [5, 6], [2, 5], [6, 7], [4, 7], [4, 7], [4, 7], [3, 5], [2, 5], [0, 4], [1, 4], [1, 3], [4, 7], [7, 0], [2, 3], [4, 7], [1, 4], [5, 4], [2, 6], [0, 4], [2, 1], [7, 2], [7, 3], [1, 4], [4, 0], [5, 2], [1, 0], [2, 5], [1, 4], [0, 7], [3, 5], [3, 6], [6, 4], [5, 7], [3, 0], [2, 3], [6, 7], [4, 7], [4, 7], [4, 7], [5, 4], [7, 0], [2, 3], [1, 0], [0, 1], [3, 5], [2, 5], [0, 1], [3, 5], [2, 5], [5, 7], [3, 5], [1, 0], [5, 4], [2, 6], [2, 5], [6, 7], [4, 7], [4, 0], [4, 7], [1, 4], [1, 5], [7, 0], [6, 2], [7, 5], [3, 2], [4, 1], [4, 0], [2, 5], [2, 5], [0, 1], [5, 2], [1, 0], [0, 7], [3, 5], [6, 2], [7, 5], [3, 2], [1, 4], [6, 7], [4, 0], [5, 7], [1, 4], [1, 7], [5, 4], [2, 5], [1, 0], [1, 5], [5, 4], [7, 4], [4, 5], [4, 1], [4, 5], [0, 4], [3, 7], [3, 2], [0, 7], [0, 7], [3, 5], [2, 5], [0, 4], [1, 4], [1, 3], [4, 7], [5, 4], [2, 3], [1, 4], [5, 0], [2, 5], [6, 7], [4, 7], [4, 0], [4, 7], [3, 5], [1, 0], [1, 4], [2, 5], [1, 4], [7, 5], [1, 4], [4, 1], [1, 0], [2, 0], [4, 7], [0, 4], [3, 5], [3, 5], [2, 5], [1, 0], [2, 6], [4, 1], [4, 0], [5, 7], [2, 5], [4, 3], [2, 5], [0, 1], [3, 5], [5, 6], [2, 5], [2, 5], [6, 7], [4, 7], [4, 0], [4, 7], [3, 5], [2, 5], [0, 4], [1, 4], [4, 7], [3, 5], [1, 0], [1, 0], [5, 7], [3, 2], [0, 2], [5, 6], [3, 5], [5, 3], [1, 5], [1, 5], [5, 1], [7, 0], [2, 5], [1, 0], [1, 0], [2, 6], [4, 0], [2, 5], [1, 0], [3, 5], [2, 5], [5, 7], [3, 2], [1, 0], [5, 7], [2, 5], [1, 0], [1, 0], [3, 5], [3, 1], [4, 1], [5, 0], [2, 5], [2, 5], [6, 7], [4, 7], [4, 0], [7, 4], [3, 5], [3, 5], [4, 6], [3, 2], [3, 2], [4, 7], [1, 0], [5, 7], [2, 4], [5, 1], [7, 0], [2, 7], [1, 0], [1, 2], [3, 5], [7, 1], [1, 7], [5, 0], [2, 5], [6, 7], [4, 7], [4, 7], [4, 0], [3, 5], [7, 6], [2, 5], [2, 5], [4, 0], [3, 2], [7, 2], [0, 1], [2, 7], [3, 5], [2, 5], [0, 4], [1, 4], [1, 3], [4, 7], [5, 4], [2, 0], [2, 5], [6, 7], [4, 7], [4, 7], [4, 2], [1, 5], [2, 3], [4, 7], [4, 1], [4, 0], [5, 2], [1, 0], [2, 0], [4, 7], [0, 7], [3, 5], [0, 7], [5, 6], [5, 6], [6, 5], [2, 5], [2, 5], [1, 0], [2, 5], [2, 5], [0, 5], [5, 6]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer(data, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m t\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:1200\u001b[0m, in \u001b[0;36mMixtralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1190\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1191\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         use_cache,\n\u001b[1;32m   1198\u001b[0m     )\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:918\u001b[0m, in \u001b[0;36mMixtralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache)\u001b[0m\n\u001b[1;32m    916\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    917\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 918\u001b[0m hidden_states, router_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_sparse_moe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    921\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:828\u001b[0m, in \u001b[0;36mMixtralSparseMoeBlock.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    826\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_dim)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# router_logits: (batch * sequence_length, n_experts)\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m router_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m routing_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(router_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    831\u001b[0m routing_weights, selected_experts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(routing_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/verb-workspace/llm-viz/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36mget_print_token.<locals>.print_tokens\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(topk_list)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, topk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(topk_list):\n\u001b[0;32m---> 11\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mpos_to_token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m     topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(topk)\n\u001b[1;32m     13\u001b[0m     token_to_expert[token]\u001b[38;5;241m.\u001b[39mappend(topk)\n",
      "\u001b[0;31mKeyError\u001b[0m: 11"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(data, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "with t.no_grad():\n",
    "  outputs = model(**tokenized.to(model.device), output_hidden_states=True, return_dict=True)\n",
    "\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Layer 0': defaultdict(set,\n",
       "             {6: {'<0x0A>', '▁', '▁='},\n",
       "              2: {'oul', '▁', '▁Robert'},\n",
       "              5: {'<0x0A>', 'oul', '▁', '▁=', '▁Robert'},\n",
       "              0: {'<0x0A>', 'ter', '▁=', '▁B'},\n",
       "              1: {'▁=', '▁B'},\n",
       "              4: {'ter'}}),\n",
       " 'Layer 1': defaultdict(set,\n",
       "             {3: {'▁'},\n",
       "              0: {'▁', '▁='},\n",
       "              2: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B'},\n",
       "              4: {'<0x0A>', 'oul', '▁=', '▁B'},\n",
       "              5: {'▁Robert'},\n",
       "              7: {'▁Robert'},\n",
       "              1: {'<0x0A>', 'ter'}}),\n",
       " 'Layer 2': defaultdict(set,\n",
       "             {7: {'<0x0A>', '▁'},\n",
       "              1: {'▁'},\n",
       "              0: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              2: {'<0x0A>', '▁=', '▁Robert'},\n",
       "              6: {'▁='},\n",
       "              4: {'oul', '▁', '▁B'},\n",
       "              5: {'ter'}}),\n",
       " 'Layer 3': defaultdict(set,\n",
       "             {6: {'ter', '▁'},\n",
       "              4: {'oul', '▁'},\n",
       "              3: {'<0x0A>', '▁='},\n",
       "              1: {'<0x0A>', '▁=', '▁B', '▁Robert'},\n",
       "              7: {'▁Robert'},\n",
       "              5: {'oul', 'ter', '▁', '▁=', '▁B'},\n",
       "              0: {'▁='}}),\n",
       " 'Layer 4': defaultdict(set,\n",
       "             {3: {'<0x0A>', '▁', '▁='},\n",
       "              0: {'<0x0A>', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              1: {'<0x0A>'},\n",
       "              2: {'oul', 'ter', '▁', '▁='},\n",
       "              4: {'▁Robert'},\n",
       "              7: {'▁B'},\n",
       "              5: {'oul', 'ter', '▁='},\n",
       "              6: {'<0x0A>'}}),\n",
       " 'Layer 5': defaultdict(set,\n",
       "             {3: {'<0x0A>', 'oul', '▁', '▁B'},\n",
       "              5: {'<0x0A>', '▁'},\n",
       "              4: {'<0x0A>'},\n",
       "              1: {'▁', '▁=', '▁B', '▁Robert'},\n",
       "              0: {'ter', '▁=', '▁Robert'},\n",
       "              2: {'oul', 'ter'},\n",
       "              7: {'▁='},\n",
       "              6: {'<0x0A>', '▁', '▁='}}),\n",
       " 'Layer 6': defaultdict(set,\n",
       "             {3: {'<0x0A>', 'oul', 'ter', '▁', '▁B', '▁Robert'},\n",
       "              2: {'<0x0A>', '▁', '▁='},\n",
       "              0: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B'},\n",
       "              7: {'<0x0A>', '▁', '▁='},\n",
       "              6: {'▁Robert'}}),\n",
       " 'Layer 7': defaultdict(set,\n",
       "             {3: {'<0x0A>', '▁'},\n",
       "              5: {'<0x0A>', 'oul', '▁', '▁=', '▁B'},\n",
       "              7: {'<0x0A>', '▁Robert'},\n",
       "              2: {'<0x0A>', '▁', '▁='},\n",
       "              1: {'oul', 'ter', '▁B', '▁Robert'},\n",
       "              6: {'ter'}}),\n",
       " 'Layer 8': defaultdict(set,\n",
       "             {7: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              4: {'<0x0A>', '▁'},\n",
       "              1: {'<0x0A>', '▁='},\n",
       "              2: {'<0x0A>', 'oul', '▁', '▁=', '▁Robert'},\n",
       "              6: {'ter', '▁B'},\n",
       "              5: {'<0x0A>', '▁'}}),\n",
       " 'Layer 9': defaultdict(set,\n",
       "             {4: {'▁'},\n",
       "              1: {'<0x0A>', '▁', '▁='},\n",
       "              2: {'<0x0A>', 'oul', 'ter', '▁B', '▁Robert'},\n",
       "              7: {'<0x0A>'},\n",
       "              5: {'<0x0A>', '▁', '▁='},\n",
       "              0: {'<0x0A>', '▁', '▁=', '▁Robert'},\n",
       "              3: {'oul', 'ter', '▁B'}}),\n",
       " 'Layer 10': defaultdict(set,\n",
       "             {6: {'▁'},\n",
       "              3: {'▁'},\n",
       "              2: {'<0x0A>', '▁', '▁='},\n",
       "              0: {'<0x0A>', '▁='},\n",
       "              5: {'▁='},\n",
       "              4: {'oul', '▁', '▁B', '▁Robert'},\n",
       "              7: {'oul', 'ter', '▁B', '▁Robert'},\n",
       "              1: {'ter'}}),\n",
       " 'Layer 11': defaultdict(set,\n",
       "             {5: {'ter', '▁'},\n",
       "              6: {'▁', '▁=', '▁Robert'},\n",
       "              2: {'<0x0A>', 'oul', 'ter', '▁B', '▁Robert'},\n",
       "              4: {'<0x0A>'},\n",
       "              0: {'▁', '▁='},\n",
       "              1: {'▁', '▁=', '▁B'},\n",
       "              7: {'oul'}}),\n",
       " 'Layer 12': defaultdict(set,\n",
       "             {7: {'▁', '▁='},\n",
       "              4: {'<0x0A>', '▁'},\n",
       "              3: {'<0x0A>', 'oul', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              5: {'▁='},\n",
       "              2: {'▁Robert'},\n",
       "              6: {'▁B'},\n",
       "              0: {'oul', 'ter'},\n",
       "              1: {'<0x0A>', 'ter'}}),\n",
       " 'Layer 13': defaultdict(set,\n",
       "             {3: {'<0x0A>', '▁', '▁='},\n",
       "              1: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              6: {'<0x0A>'},\n",
       "              0: {'oul', '▁B', '▁Robert'},\n",
       "              2: {'<0x0A>', 'ter'},\n",
       "              7: {'<0x0A>'}}),\n",
       " 'Layer 14': defaultdict(set,\n",
       "             {5: {'▁', '▁Robert'},\n",
       "              7: {'<0x0A>', '▁', '▁=', '▁B'},\n",
       "              2: {'<0x0A>', 'oul'},\n",
       "              4: {'<0x0A>', '▁', '▁='},\n",
       "              6: {'oul', 'ter', '▁B', '▁Robert'},\n",
       "              3: {'ter'},\n",
       "              0: {'<0x0A>', '▁', '▁='},\n",
       "              1: {'<0x0A>', '▁='}}),\n",
       " 'Layer 15': defaultdict(set,\n",
       "             {4: {'<0x0A>', '▁', '▁Robert'},\n",
       "              6: {'▁', '▁=', '▁B'},\n",
       "              1: {'<0x0A>', '▁'},\n",
       "              2: {'▁='},\n",
       "              5: {'<0x0A>', 'oul', '▁=', '▁B', '▁Robert'},\n",
       "              3: {'oul'},\n",
       "              7: {'ter'},\n",
       "              0: {'<0x0A>', 'ter', '▁', '▁='}}),\n",
       " 'Layer 16': defaultdict(set,\n",
       "             {1: {'oul', 'ter', '▁', '▁Robert'},\n",
       "              3: {'<0x0A>', '▁'},\n",
       "              7: {'<0x0A>', '▁', '▁='},\n",
       "              4: {'<0x0A>', '▁='},\n",
       "              5: {'<0x0A>', '▁='},\n",
       "              2: {'oul', 'ter', '▁B', '▁Robert'},\n",
       "              6: {'▁', '▁B'}}),\n",
       " 'Layer 17': defaultdict(set,\n",
       "             {1: {'▁', '▁='},\n",
       "              5: {'ter', '▁', '▁='},\n",
       "              0: {'<0x0A>'},\n",
       "              7: {'<0x0A>', '▁', '▁='},\n",
       "              6: {'<0x0A>', '▁B', '▁Robert'},\n",
       "              3: {'oul', '▁B', '▁Robert'},\n",
       "              4: {'oul'},\n",
       "              2: {'ter'}}),\n",
       " 'Layer 18': defaultdict(set,\n",
       "             {6: {'▁', '▁='},\n",
       "              3: {'▁', '▁='},\n",
       "              0: {'<0x0A>', '▁B'},\n",
       "              4: {'<0x0A>', 'ter'},\n",
       "              1: {'▁', '▁='},\n",
       "              5: {'oul', '▁Robert'},\n",
       "              7: {'<0x0A>', 'oul', '▁Robert'},\n",
       "              2: {'ter', '▁B'}}),\n",
       " 'Layer 19': defaultdict(set,\n",
       "             {6: {'<0x0A>', '▁', '▁Robert'},\n",
       "              2: {'<0x0A>', '▁'},\n",
       "              1: {'oul', 'ter', '▁=', '▁B', '▁Robert'},\n",
       "              4: {'▁='},\n",
       "              7: {'<0x0A>', 'oul', '▁B'},\n",
       "              0: {'<0x0A>', 'ter', '▁', '▁='},\n",
       "              3: {'▁'}}),\n",
       " 'Layer 20': defaultdict(set,\n",
       "             {0: {'<0x0A>', '▁'},\n",
       "              5: {'ter', '▁', '▁='},\n",
       "              2: {'<0x0A>', '▁', '▁='},\n",
       "              6: {'<0x0A>', '▁='},\n",
       "              4: {'oul', '▁Robert'},\n",
       "              1: {'<0x0A>', 'oul', '▁Robert'},\n",
       "              3: {'▁B'},\n",
       "              7: {'<0x0A>', 'ter', '▁', '▁B'}}),\n",
       " 'Layer 21': defaultdict(set,\n",
       "             {0: {'<0x0A>', '▁'},\n",
       "              3: {'ter', '▁', '▁='},\n",
       "              7: {'<0x0A>', 'oul', '▁', '▁Robert'},\n",
       "              2: {'▁=', '▁B'},\n",
       "              5: {'oul', '▁='},\n",
       "              1: {'<0x0A>', 'ter', '▁B', '▁Robert'}}),\n",
       " 'Layer 22': defaultdict(set,\n",
       "             {1: {'<0x0A>', '▁'},\n",
       "              0: {'<0x0A>', 'oul', 'ter', '▁', '▁B'},\n",
       "              2: {'<0x0A>', '▁='},\n",
       "              6: {'oul', '▁=', '▁B'},\n",
       "              7: {'▁Robert'},\n",
       "              5: {'▁Robert'},\n",
       "              4: {'<0x0A>', 'ter', '▁='}}),\n",
       " 'Layer 23': defaultdict(set,\n",
       "             {3: {'<0x0A>', 'oul', '▁', '▁B', '▁Robert'},\n",
       "              1: {'▁'},\n",
       "              4: {'<0x0A>', 'ter', '▁', '▁=', '▁Robert'},\n",
       "              7: {'▁='},\n",
       "              6: {'<0x0A>', 'ter', '▁', '▁=', '▁B'},\n",
       "              5: {'oul'}}),\n",
       " 'Layer 24': defaultdict(set,\n",
       "             {6: {'▁'},\n",
       "              1: {'▁'},\n",
       "              2: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              3: {'<0x0A>', 'oul', '▁=', '▁B'},\n",
       "              5: {'<0x0A>', '▁=', '▁Robert'},\n",
       "              7: {'<0x0A>', 'ter'}}),\n",
       " 'Layer 25': defaultdict(set,\n",
       "             {6: {'<0x0A>', '▁', '▁=', '▁B'},\n",
       "              1: {'<0x0A>', 'oul', 'ter', '▁', '▁=', '▁B'},\n",
       "              2: {'<0x0A>'},\n",
       "              0: {'▁', '▁='},\n",
       "              3: {'▁Robert'},\n",
       "              5: {'▁Robert'},\n",
       "              7: {'oul'},\n",
       "              4: {'ter'}}),\n",
       " 'Layer 26': defaultdict(set,\n",
       "             {4: {'▁', '▁Robert'},\n",
       "              2: {'<0x0A>', 'oul', '▁', '▁=', '▁B'},\n",
       "              0: {'<0x0A>', '▁', '▁='},\n",
       "              7: {'▁Robert'},\n",
       "              5: {'<0x0A>', '▁=', '▁B'},\n",
       "              1: {'oul', 'ter'},\n",
       "              6: {'ter'}}),\n",
       " 'Layer 27': defaultdict(set,\n",
       "             {5: {'▁', '▁B'},\n",
       "              2: {'oul', 'ter', '▁', '▁B'},\n",
       "              1: {'<0x0A>', 'oul', '▁'},\n",
       "              7: {'<0x0A>', '▁', '▁=', '▁Robert'},\n",
       "              3: {'<0x0A>', '▁='},\n",
       "              6: {'▁Robert'},\n",
       "              0: {'<0x0A>', 'ter'}}),\n",
       " 'Layer 28': defaultdict(set,\n",
       "             {2: {'▁', '▁=', '▁Robert'},\n",
       "              4: {'▁', '▁Robert'},\n",
       "              3: {'<0x0A>', '▁', '▁=', '▁B'},\n",
       "              5: {'<0x0A>', 'ter'},\n",
       "              0: {'oul', '▁='},\n",
       "              6: {'▁', '▁B'},\n",
       "              1: {'oul', 'ter'}}),\n",
       " 'Layer 29': defaultdict(set,\n",
       "             {3: {'<0x0A>', '▁'},\n",
       "              4: {'<0x0A>', 'ter', '▁', '▁='},\n",
       "              5: {'<0x0A>', '▁', '▁=', '▁B', '▁Robert'},\n",
       "              2: {'▁=', '▁Robert'},\n",
       "              1: {'ter', '▁B'},\n",
       "              7: {'oul', '▁'},\n",
       "              6: {'<0x0A>', 'oul'}}),\n",
       " 'Layer 30': defaultdict(set,\n",
       "             {4: {'<0x0A>', '▁', '▁Robert'},\n",
       "              0: {'<0x0A>', 'oul', '▁', '▁='},\n",
       "              5: {'<0x0A>', '▁B'},\n",
       "              7: {'oul', 'ter', '▁', '▁='},\n",
       "              2: {'ter', '▁Robert'},\n",
       "              3: {'▁B'}}),\n",
       " 'Layer 31': defaultdict(set,\n",
       "             {2: {'<0x0A>', 'ter', '▁', '▁='},\n",
       "              7: {'<0x0A>', '▁', '▁='},\n",
       "              6: {'▁='},\n",
       "              5: {'oul', '▁B', '▁Robert'},\n",
       "              1: {'oul', '▁B', '▁Robert'},\n",
       "              0: {'ter'}})}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in hooks:\n",
    "    handle.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-viz-5M-np4rU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
