{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Usage for Mixtral 8x7B\n",
    "\n",
    "After experimenting with the expert hooks in Mixtral 8x7B, I want to see how many tokens of different datasets are sent to which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(os.getenv(\"HF_TOKEN\"), add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "I will be using 3 different datasets, and recording the percentages of tokens hanldled by each of the experts in the first and the last layers of the model. Let's see how this differs for different datasets. The datasets are\n",
    "- `stanfordnlp/imdb` - this is a classification task for movie reviews.\n",
    "- `databricks/databricks-dolly-15k` - plain 'ol question answering\n",
    "- `bigcode/bigcodebench` - code generation from prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
    "qa_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "code_dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.0_hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [02:17<00:00,  7.24s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixtralModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MixtralDecoderLayer(\n",
      "      (self_attn): MixtralSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MixtralRotaryEmbedding()\n",
      "      )\n",
      "      (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "        (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
      "        (experts): ModuleList(\n",
      "          (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
      "            (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (input_layernorm): MixtralRMSNorm()\n",
      "      (post_attention_layernorm): MixtralRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): MixtralRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_LAYERS = len(model.layers)\n",
    "NUM_EXPERTS = len(model.layers[0].block_sparse_moe.experts)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_usage = defaultdict(int)\n",
    "last_layer_usage = defaultdict(int)\n",
    "\n",
    "# these two will store the values for all the datasets \n",
    "all_first_layer_usages = dict()\n",
    "all_last_layer_usages = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_layer_update(module, input, output):\n",
    "    _, topk_index = t.topk(output, 2, dim=1) \n",
    "    # topk_list of of the shape [S_l, 2] where S_l is the length of the sequence\n",
    "    topk_list = topk_index.tolist()\n",
    "\n",
    "    # iterate over all the tokens in the sequence\n",
    "    for topk in topk_list: \n",
    "        expert_1, expert_2 = tuple(topk) \n",
    "        first_layer_usage[expert_1] += 1\n",
    "        first_layer_usage[expert_2] += 1\n",
    "\n",
    "def last_layer_update(module, input, output): \n",
    "    _, topk_index = t.topk(output, 2, dim=1) \n",
    "    # topk_list of of the shape [S_l, 2] where S_l is the length of the sequence\n",
    "    topk_list = topk_index.tolist()\n",
    "\n",
    "    # iterate over all the tokens in the sequence\n",
    "    for topk in topk_list: \n",
    "        expert_1, expert_2 = tuple(topk) \n",
    "        last_layer_usage[expert_1] += 1\n",
    "        last_layer_usage[expert_2] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will only register the hooks for the first and the last experts. We can try and visualize these then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []\n",
    "hooks.append(model.layers[0].block_sparse_moe.gate.register_forward_hook(first_layer_update))\n",
    "hooks.append(model.layers[-1].block_sparse_moe.gate.register_forward_hook(last_layer_update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(usage: dict, total_num_tokens: int) -> dict:\n",
    "    norm_usage = usage.copy()\n",
    "    for expert_num, expert_usage in usage.items():\n",
    "        norm_usage[expert_num] = expert_usage / total_num_tokens\n",
    "    \n",
    "    return norm_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 5000/5000 [23:25<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "first_layer_usage = defaultdict(int)\n",
    "last_layer_usage = defaultdict(int)\n",
    "\n",
    "dataset = qa_dataset[\"instruction\"][:5000]\n",
    "num_tokens = tokenizer(\"\".join(dataset), return_length=True).length[0]\n",
    "for instruction in tqdm(dataset):\n",
    "    tok_instruction = tokenizer(instruction, return_tensors=\"pt\")\n",
    "    outputs = model(**tok_instruction)\n",
    "\n",
    "all_first_layer_usages[\"qa\"] = normalize_dataset(first_layer_usage, num_tokens)\n",
    "all_last_layer_usages[\"qa\"] = normalize_dataset(last_layer_usage, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1140/1140 [05:38<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "first_layer_usage = defaultdict(int)\n",
    "last_layer_usage = defaultdict(int)\n",
    "\n",
    "dataset = code_dataset[\"instruct_prompt\"]\n",
    "num_tokens = tokenizer(\"\".join(dataset), return_length=True).length[0]\n",
    "for instruction in tqdm(dataset):\n",
    "    tok_instruction = tokenizer(instruction, return_tensors=\"pt\")\n",
    "    outputs = model(**tok_instruction) \n",
    "\n",
    "all_first_layer_usages[\"code\"] = normalize_dataset(first_layer_usage, num_tokens)\n",
    "all_last_layer_usages[\"code\"] = normalize_dataset(last_layer_usage, num_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [24:57<00:00,  3.34it/s]\n"
     ]
    }
   ],
   "source": [
    "first_layer_usage = defaultdict(int)\n",
    "last_layer_usage = defaultdict(int)\n",
    "\n",
    "dataset = imdb_dataset[\"text\"][:5000]\n",
    "num_tokens = tokenizer(\"\".join(dataset), return_length=True).length[0]\n",
    "for review in tqdm(dataset):\n",
    "    preprompt = \"Classify this as a negative or positive review\"\n",
    "    tok_instruction = tokenizer(f\"{preprompt}:{instruction}\", return_tensors=\"pt\")\n",
    "    outputs = model(**tok_instruction)    \n",
    "\n",
    "all_first_layer_usages[\"imdb\"] = normalize_dataset(first_layer_usage, num_tokens)\n",
    "all_last_layer_usages[\"imbd\"] = normalize_dataset(last_layer_usage, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting \n",
    "\n",
    "Now we can plot these usage charts per dataset. The first and the last layers will be printed seperately. Essentially I want to see how much each expert gets used in each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usage_chart(usage: dict):\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    data = []\n",
    "    for category, values in usage.items():\n",
    "        for key, value in values.items():\n",
    "            data.append({\"Category\": category, \"Key\": key, \"Value\": value})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create the Altair chart\n",
    "    chart = alt.Chart(df).mark_bar().encode(\n",
    "        x=alt.X('Key:O', title='Keys'),\n",
    "        y=alt.Y('Value:Q', title='Values'),\n",
    "        color=alt.Color('Category:N', scale=alt.Scale(scheme='category10')),\n",
    "        column=alt.Column('Category:N', title=None)\n",
    "    ).properties(\n",
    "        width=300,\n",
    "        height=400,\n",
    "        title='Values for Each Category'\n",
    "    )\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_chart = get_usage_chart(all_first_layer_usages)\n",
    "last_layer_chart = get_usage_chart(all_last_layer_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-32aee91b5bbc47f9b26f9017e924743d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-32aee91b5bbc47f9b26f9017e924743d.vega-embed details,\n",
       "  #altair-viz-32aee91b5bbc47f9b26f9017e924743d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-32aee91b5bbc47f9b26f9017e924743d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-32aee91b5bbc47f9b26f9017e924743d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-32aee91b5bbc47f9b26f9017e924743d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2f87e67c80b4febd5e8f04beea6af400\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Category\", \"scale\": {\"scheme\": \"category10\"}, \"type\": \"nominal\"}, \"column\": {\"field\": \"Category\", \"title\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"Key\", \"title\": \"Keys\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"Value\", \"title\": \"Values\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Values for Each Category\", \"width\": 300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-2f87e67c80b4febd5e8f04beea6af400\": [{\"Category\": \"qa\", \"Key\": 5, \"Value\": 0.3618814227421424}, {\"Category\": \"qa\", \"Key\": 1, \"Value\": 0.3223248448138344}, {\"Category\": \"qa\", \"Key\": 3, \"Value\": 0.2745610283323122}, {\"Category\": \"qa\", \"Key\": 6, \"Value\": 0.2141164503115283}, {\"Category\": \"qa\", \"Key\": 4, \"Value\": 0.22724803199667087}, {\"Category\": \"qa\", \"Key\": 7, \"Value\": 0.20292686310094904}, {\"Category\": \"qa\", \"Key\": 0, \"Value\": 0.22143360806390086}, {\"Category\": \"qa\", \"Key\": 2, \"Value\": 0.2816469962662844}, {\"Category\": \"code\", \"Key\": 5, \"Value\": 0.2702016058865936}, {\"Category\": \"code\", \"Key\": 1, \"Value\": 0.24511397817386332}, {\"Category\": \"code\", \"Key\": 3, \"Value\": 0.29750167797747784}, {\"Category\": \"code\", \"Key\": 2, \"Value\": 0.2214532527904144}, {\"Category\": \"code\", \"Key\": 7, \"Value\": 0.23366395704377657}, {\"Category\": \"code\", \"Key\": 4, \"Value\": 0.28327740075073954}, {\"Category\": \"code\", \"Key\": 0, \"Value\": 0.24415442364581003}, {\"Category\": \"code\", \"Key\": 6, \"Value\": 0.2164963830263256}, {\"Category\": \"imdb\", \"Key\": 5, \"Value\": 0.11403346502086813}, {\"Category\": \"imdb\", \"Key\": 1, \"Value\": 0.15521221627840384}, {\"Category\": \"imdb\", \"Key\": 3, \"Value\": 0.16154740877956317}, {\"Category\": \"imdb\", \"Key\": 6, \"Value\": 0.11403346502086813}, {\"Category\": \"imdb\", \"Key\": 7, \"Value\": 0.17105019753130218}, {\"Category\": \"imdb\", \"Key\": 4, \"Value\": 0.1362066387749258}, {\"Category\": \"imdb\", \"Key\": 0, \"Value\": 0.18372058253362086}, {\"Category\": \"imdb\", \"Key\": 2, \"Value\": 0.10453067626912911}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_layer_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-6fb0cc6850d54ad8b7342c10aacd1f58.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-6fb0cc6850d54ad8b7342c10aacd1f58.vega-embed details,\n",
       "  #altair-viz-6fb0cc6850d54ad8b7342c10aacd1f58.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-6fb0cc6850d54ad8b7342c10aacd1f58\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6fb0cc6850d54ad8b7342c10aacd1f58\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6fb0cc6850d54ad8b7342c10aacd1f58\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-34668fec563cb3e9758dd6894480a7d7\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Category\", \"scale\": {\"scheme\": \"category10\"}, \"type\": \"nominal\"}, \"column\": {\"field\": \"Category\", \"title\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"Key\", \"title\": \"Keys\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"Value\", \"title\": \"Values\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Values for Each Category\", \"width\": 300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-34668fec563cb3e9758dd6894480a7d7\": [{\"Category\": \"qa\", \"Key\": 2, \"Value\": 0.32292593834167543}, {\"Category\": \"qa\", \"Key\": 7, \"Value\": 0.3777063658116497}, {\"Category\": \"qa\", \"Key\": 1, \"Value\": 0.2975644152631518}, {\"Category\": \"qa\", \"Key\": 5, \"Value\": 0.32693708169092234}, {\"Category\": \"qa\", \"Key\": 6, \"Value\": 0.19329780716457248}, {\"Category\": \"qa\", \"Key\": 3, \"Value\": 0.27349755516767044}, {\"Category\": \"qa\", \"Key\": 4, \"Value\": 0.118022402293403}, {\"Category\": \"qa\", \"Key\": 0, \"Value\": 0.19618767989457744}, {\"Category\": \"code\", \"Key\": 2, \"Value\": 0.20872548288462972}, {\"Category\": \"code\", \"Key\": 7, \"Value\": 0.21097272975861983}, {\"Category\": \"code\", \"Key\": 1, \"Value\": 0.26100877520073584}, {\"Category\": \"code\", \"Key\": 5, \"Value\": 0.28125388420712455}, {\"Category\": \"code\", \"Key\": 6, \"Value\": 0.3020608049320108}, {\"Category\": \"code\", \"Key\": 3, \"Value\": 0.36845899520222736}, {\"Category\": \"code\", \"Key\": 4, \"Value\": 0.1471996420314714}, {\"Category\": \"code\", \"Key\": 0, \"Value\": 0.23218236507818132}, {\"Category\": \"imbd\", \"Key\": 2, \"Value\": 0.09186029126681043}, {\"Category\": \"imbd\", \"Key\": 7, \"Value\": 0.09186029126681043}, {\"Category\": \"imbd\", \"Key\": 1, \"Value\": 0.14254183127608516}, {\"Category\": \"imbd\", \"Key\": 5, \"Value\": 0.17421779378188185}, {\"Category\": \"imbd\", \"Key\": 6, \"Value\": 0.16471500503014286}, {\"Category\": \"imbd\", \"Key\": 3, \"Value\": 0.26291048879811263}, {\"Category\": \"imbd\", \"Key\": 0, \"Value\": 0.11720106127144779}, {\"Category\": \"imbd\", \"Key\": 4, \"Value\": 0.09502788751739011}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_layer_chart.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-viz-5M-np4rU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
