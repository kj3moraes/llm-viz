# Attention Maps Visualization

This experiment will be in visualizing attention maps so that we can see how fine-tuning / training helps a model pay attention to lifferent ktokens.

## Mixtral Experiments

These experiments will lead up to my final experimentation with Mixtral's 'experts' and seeing what the experts learn.

## References

- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/#attention-visualization)
- [NLG With GPT2](https://jaketae.github.io/study/gpt2/)
- [GPT2 HuggingFace Documentation](https://huggingface.co/docs/transformers/en/model_doc/gpt2)
